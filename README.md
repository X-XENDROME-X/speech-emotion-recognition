<h1 align="center">
 Speech Emotion Recognition
</h1>

<p align="center">
 <img src="img.jpg" alt="Speech Emotion Recognition" width="400"/>
</p>

A deep learning project that classifies eight emotions from speech audio using a Convolutional Neural Network (CNN). Trained on the RAVDESS dataset, this repository demonstrates end-to-end audio processingâ€”from feature extraction to model inferenceâ€”ideal for applications like virtual assistants, mental health monitoring, and human-computer interaction.

---

## ğŸš€ Table of Contents

* [âœ¨ Features](#-features)
* [ğŸ—‚ï¸ Project Structure](#ï¸-project-structure)
* [ğŸ› ï¸ Installation](#ï¸-installation)
* [â–¶ï¸ Usage](#ï¸-usage)
* [ğŸ“Š Results & Visualizations](#-results--visualizations)
* [ğŸ”® Future Improvements](#-future-improvements)
* [ğŸ¤ Contributing](#-contributing)
* [ğŸ“„ License](#-license)

---

## âœ¨ Features

* **Dataset**: RAVDESS audio dataset (1,440 samples across 8 emotions).
* **Feature Extraction**:

  * 231-dimensional feature vectors
  * MFCCs, Chroma, Mel Spectrograms, Zero-Crossing Rate, Spectral Contrast
* **Model Architecture**: CNN built in PyTorch with:

  * 2 Convolutional Layers
  * Batch Normalization & Dropout
  * Fully Connected Layers for classification
* **Performance**:

  * Single CNN: \~72.5% accuracy
  * Ensemble of 3 CNNs: \~74.2% accuracy
* **Prediction**: Function to infer emotion from new audio files

---

## ğŸ—‚ï¸ Project Structure

```bash
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ img.jpg
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ SpeechEmotionCNN.ipynb

```

* **`.gitignore`**: Excludes caches and checkpoints
* **`LICENSE`**: MIT open-source license
* **`README.md`**: Project documentation (this file)
* **`img.jpg`**: Sample process diagram
* **`requirements.txt`**: For project dependencies
* **`SpeechEmotionCNN.ipynb`**: Notebook covering data loading, feature extraction, training, evaluation, and prediction

---

## ğŸ› ï¸ Installation

1. **Clone the repository**

   ```bash
   git clone https://github.com/X-XENDROME-X/speech-emotion-recognition.git
   cd speech-emotion-recognition
   ```

2. **Set up a virtual environment (optional but recommended)**

   ```bash
   python3 -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**

   ```bash
   pip install -r requirements.txt
   ```

4. **Prepare the RAVDESS dataset**

   * The notebook will auto-download from Zenodo if `./data/ravdess/` is empty
   * Or [download manually](https://zenodo.org/record/1188976) and unzip into `./data/ravdess/`

---

## â–¶ï¸ Usage

1. **Launch Jupyter Notebook**

   ```bash
   jupyter notebook SpeechEmotionCNN.ipynb
   ```

2. **Run through the cells**:

   * Load and preprocess audio files
   * Extract features with Librosa
   * Train CNN model(s)
   * Evaluate and visualize results
   * Save model weights and label encoder

3. **Predict on new audio**

   ```python
   from predict import predict_emotion_pytorch

   model_path = 'speech_emotion_model_pytorch.pth'
   encoder_path = 'label_encoder.joblib'
   audio_file = 'path/to/audio.wav'

   predicted_emotion = predict_emotion_pytorch(
       audio_file,
       model_path,
       encoder_path,
       device='cpu'
   )
   print(f"Predicted Emotion: {predicted_emotion}")
   ```

---

## ğŸ“Š Results & Visualizations

* **Accuracy**: 72.5% (single model) â” 74.2% (ensemble)
* **Insights**:

  * High accuracy on distinct emotions (e.g., angry, happy)
  * Lower performance for neutral/calm due to class imbalance (addressed with weighted loss)
* **Visuals**:

  * Waveform plots
  * Confusion matrices
  * Training & validation accuracy/loss curves

---

## ğŸ”® Future Improvements

* Experiment with Transformer-based architectures (e.g., Audio Spectrogram Transformers)
* Integrate pre-trained audio networks (e.g., Wav2Vec)
* Add extra features: pitch, prosody, formants
* Test & fine-tune on other datasets (e.g., TESS, CREMA-D)
* Deploy as a REST API or in a real-time application

---

## ğŸ¤ Contributing

Contributions are welcome! To contribute:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature-name`)
3. Commit your changes (`git commit -m 'Add new feature'`)
4. Push to the branch (`git push origin feature-name`)
5. Open a Pull Request

Please adhere to the existing code style and include tests where applicable.

---

## ğŸ“„ License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.
